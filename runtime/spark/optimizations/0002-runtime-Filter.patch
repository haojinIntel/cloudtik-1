From 01a0c4406e7f38839bdcdf260687d6153cd1ca3f Mon Sep 17 00:00:00 2001
From: haojin <hao.jin@intel.com>
Date: Thu, 1 Sep 2022 10:30:48 +0800
Subject: [PATCH] Support more scenarios for runtime Filter

---
 sql/catalyst/pom.xml                          |   5 +
 .../optimizer/InjectRuntimeFilter.scala       | 110 ++++++++++++++++--
 .../catalyst/plans/logical/LogicalPlan.scala  |   1 +
 .../apache/spark/sql/internal/SQLConf.scala   |  16 +++
 .../datasources/HadoopFsRelation.scala        |   5 +
 .../datasources/LogicalRelation.scala         |   5 +
 .../PartitioningAwareFileIndex.scala          |   4 +-
 .../apache/spark/sql/sources/interfaces.scala |   2 +
 8 files changed, 138 insertions(+), 10 deletions(-)

diff --git a/sql/catalyst/pom.xml b/sql/catalyst/pom.xml
index ceba171e41..771bab59d3 100644
--- a/sql/catalyst/pom.xml
+++ b/sql/catalyst/pom.xml
@@ -44,6 +44,11 @@
       <artifactId>scala-parser-combinators_${scala.binary.version}</artifactId>
     </dependency>
 
+    <dependency>
+      <groupId>org.apache.spark</groupId>
+      <artifactId>spark-sql_${scala.binary.version}</artifactId>
+      <version>${project.version}</version>
+    </dependency>
     <dependency>
       <groupId>org.apache.spark</groupId>
       <artifactId>spark-core_${scala.binary.version}</artifactId>
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InjectRuntimeFilter.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InjectRuntimeFilter.scala
index 134292ae30..885ffe66e1 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InjectRuntimeFilter.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InjectRuntimeFilter.scala
@@ -17,12 +17,15 @@
 
 package org.apache.spark.sql.catalyst.optimizer
 
+import scala.collection.mutable.ArrayBuffer
+
 import org.apache.spark.sql.catalyst.expressions._
 import org.apache.spark.sql.catalyst.expressions.aggregate.{AggregateExpression, BloomFilterAggregate, Complete}
 import org.apache.spark.sql.catalyst.planning.{ExtractEquiJoinKeys, PhysicalOperation}
 import org.apache.spark.sql.catalyst.plans.logical._
 import org.apache.spark.sql.catalyst.rules.Rule
 import org.apache.spark.sql.catalyst.trees.TreePattern.{INVOKE, JSON_TO_STRUCT, LIKE_FAMLIY, PYTHON_UDF, REGEXP_EXTRACT_FAMILY, REGEXP_REPLACE, SCALA_UDF}
+import org.apache.spark.sql.execution.datasources.LogicalRelation
 import org.apache.spark.sql.internal.SQLConf
 import org.apache.spark.sql.types._
 
@@ -48,19 +51,29 @@ object InjectRuntimeFilter extends Rule[LogicalPlan] with PredicateHelper with J
       filterCreationSideExp: Expression,
       filterCreationSidePlan: LogicalPlan): LogicalPlan = {
     require(conf.runtimeFilterBloomFilterEnabled || conf.runtimeFilterSemiJoinReductionEnabled)
+    var newFilterCreationSidePlan = filterCreationSidePlan
+    var newFilterCreationSideExp = filterCreationSideExp
+    if (!isSelectiveFilterOverScan(filterCreationSidePlan)) {
+      val projectExpress = findOriginProject(
+        filterCreationSidePlan, filterCreationSideExp).
+        getOrElse().asInstanceOf[(LogicalPlan, Expression)]
+      newFilterCreationSidePlan = projectExpress._1
+      newFilterCreationSideExp = projectExpress._2
+    }
+
     if (conf.runtimeFilterBloomFilterEnabled) {
       injectBloomFilter(
         filterApplicationSideExp,
         filterApplicationSidePlan,
-        filterCreationSideExp,
-        filterCreationSidePlan
+        newFilterCreationSideExp,
+        newFilterCreationSidePlan
       )
     } else {
       injectInSubqueryFilter(
         filterApplicationSideExp,
         filterApplicationSidePlan,
-        filterCreationSideExp,
-        filterCreationSidePlan
+        newFilterCreationSideExp,
+        newFilterCreationSidePlan
       )
     }
   }
@@ -123,9 +136,41 @@ object InjectRuntimeFilter extends Rule[LogicalPlan] with PredicateHelper with J
           filters.exists(isLikelySelective)
       case _ => false
     }
+
     !plan.isStreaming && ret
   }
 
+  private def findOriginProject(
+      plan: LogicalPlan, expression: Expression):
+        Option[(LogicalPlan, Expression)] = {
+    val planWithExp = findPlanWithExp(
+      expression, plan, ArrayBuffer[(LogicalPlan, Expression)]())
+    if (planWithExp.isDefined) {
+      val path = planWithExp.asInstanceOf[ArrayBuffer[(LogicalPlan, Expression)]]
+      for (i <- (0 until path.size).reverse) {
+        if (path(i)._1.isInstanceOf[Project]) {
+          return Some(path(i))
+        }
+      }
+      None
+    }
+    None
+  }
+
+  private def isOriginProjectSelectiveFilterOverScan(
+      plan: LogicalPlan, expression: Expression): Boolean = {
+    if (conf.getConf(SQLConf.RUNTIME_FILTER_CREATION_SIDE_EXTEND_ENABLED)) {
+      val projectPlanAndExp = findOriginProject(plan, expression)
+      if (projectPlanAndExp.isDefined) {
+        val projectPlan = projectPlanAndExp.getOrElse().asInstanceOf[(LogicalPlan, Expression)]._1
+        return isSelectiveFilterOverScan(projectPlan)
+      } else {
+        return false
+      }
+    }
+    false
+  }
+
   private def isSimpleExpression(e: Expression): Boolean = {
     !e.containsAnyPattern(PYTHON_UDF, SCALA_UDF, INVOKE, JSON_TO_STRUCT, LIKE_FAMLIY,
       REGEXP_EXTRACT_FAMILY, REGEXP_REPLACE)
@@ -155,7 +200,22 @@ object InjectRuntimeFilter extends Rule[LogicalPlan] with PredicateHelper with J
       // creating a Bloom filter when the filter application side is very small, so using 0
       // as the byte size when the actual size is unknown can avoid regression by applying BF
       // on a small table.
-      if (scan.stats.sizeInBytes == defaultSizeInBytes) BigInt(0) else scan.stats.sizeInBytes
+      if (scan.stats.sizeInBytes == defaultSizeInBytes) {
+        if (conf.getConf(SQLConf.RUNTIME_FILTER_ESTIMATE_STATS_ENABLED)) {
+          if (scan.isInstanceOf[LogicalRelation]) {
+            val estimateSizeInBytes =
+              scan.asInstanceOf[LogicalRelation].estimateStats().sizeInBytes
+            if (estimateSizeInBytes == defaultSizeInBytes) BigInt(0) else estimateSizeInBytes
+          }
+          else {
+            BigInt(0)
+          }
+        } else {
+          BigInt(0)
+        }
+      } else {
+        scan.stats.sizeInBytes
+      }
     }).max
   }
 
@@ -182,9 +242,12 @@ object InjectRuntimeFilter extends Rule[LogicalPlan] with PredicateHelper with J
       filterApplicationSide: LogicalPlan,
       filterCreationSide: LogicalPlan,
       filterApplicationSideExp: Expression,
+      filterCreationSideSideExp: Expression,
       hint: JoinHint): Boolean = {
     findExpressionAndTrackLineageDown(filterApplicationSideExp,
-      filterApplicationSide).isDefined && isSelectiveFilterOverScan(filterCreationSide) &&
+      filterApplicationSide).isDefined &&
+      (isSelectiveFilterOverScan(filterCreationSide) ||
+        isOriginProjectSelectiveFilterOverScan(filterCreationSide, filterCreationSideSideExp)) &&
       (isProbablyShuffleJoin(filterApplicationSide, filterCreationSide, hint) ||
         probablyHasShuffle(filterApplicationSide)) &&
       satisfyByteSizeRequirement(filterApplicationSide)
@@ -235,6 +298,37 @@ object InjectRuntimeFilter extends Rule[LogicalPlan] with PredicateHelper with J
     }
   }
 
+  private def findPlanWithExp(
+    exp: Expression, plan: LogicalPlan, path: ArrayBuffer[(LogicalPlan, Expression)])
+        : Option[ArrayBuffer[(LogicalPlan, Expression)]] = {
+    if (exp.references.isEmpty) return None
+    plan match {
+      case p: Project =>
+        val aliases = getAliasMap(p)
+        val exp_aliases = replaceAlias(exp, aliases)
+        path += Tuple2(plan, exp_aliases)
+        findPlanWithExp(exp_aliases, p.child, path)
+      // we can unwrap only if there are row projections, and no aggregation operation
+      case a: Aggregate =>
+        val aliasMap = getAliasMap(a)
+        val exp_aliases = replaceAlias(exp, aliasMap)
+        path += Tuple2(plan, exp_aliases)
+        findPlanWithExp(exp_aliases, a.child, path)
+      case l: LeafNode if exp.references.subsetOf(l.outputSet) =>
+        path += Tuple2(plan, exp)
+        Some(path)
+      case other =>
+        path += Tuple2(plan, exp)
+        other.children.flatMap {
+          child => if (exp.references.subsetOf(child.outputSet)) {
+            findPlanWithExp(exp, child, path)
+          } else {
+            None
+          }
+        }.headOption
+    }
+  }
+
   def hasInSubquery(left: LogicalPlan, right: LogicalPlan, leftKey: Expression,
       rightKey: Expression): Boolean = {
     (left, right) match {
@@ -266,12 +360,12 @@ object InjectRuntimeFilter extends Rule[LogicalPlan] with PredicateHelper with J
             isSimpleExpression(l) && isSimpleExpression(r)) {
             val oldLeft = newLeft
             val oldRight = newRight
-            if (canPruneLeft(joinType) && filteringHasBenefit(left, right, l, hint)) {
+            if (canPruneLeft(joinType) && filteringHasBenefit(left, right, l, r, hint)) {
               newLeft = injectFilter(l, newLeft, r, right)
             }
             // Did we actually inject on the left? If not, try on the right
             if (newLeft.fastEquals(oldLeft) && canPruneRight(joinType) &&
-              filteringHasBenefit(right, left, r, hint)) {
+              filteringHasBenefit(right, left, r, l, hint)) {
               newRight = injectFilter(r, newRight, l, left)
             }
             if (!newLeft.fastEquals(oldLeft) || !newRight.fastEquals(oldRight)) {
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala
index 7640d9234c..6bf5db6d55 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala
@@ -169,6 +169,7 @@ trait LeafNode extends LogicalPlan with LeafLike[LogicalPlan] {
 
   /** Leaf nodes that can survive analysis must define their own statistics. */
   def computeStats(): Statistics = throw new UnsupportedOperationException
+
 }
 
 /**
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
index 6df880bd4e..8812a341bd 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
@@ -341,6 +341,22 @@ object SQLConf {
       .booleanConf
       .createWithDefault(true)
 
+  val RUNTIME_FILTER_CREATION_SIDE_EXTEND_ENABLED =
+    buildConf("spark.sql.optimizer.runtimeFilter.creationSideExtend.enabled")
+      .doc("When true and if the plan is not a simple filter over scan, we attempt " +
+        "to get the origin likely selective plan according to exp.")
+      .version("3.3.0")
+      .booleanConf
+      .createWithDefault(false)
+
+  val RUNTIME_FILTER_ESTIMATE_STATS_ENABLED =
+    buildConf("spark.sql.optimizer.runtimeFilter.estimateStats.enabled")
+      .doc("When true and if sizeInBytes of the plan is unknown, we attempt " +
+        "to estimate the sizeInBytes.")
+      .version("3.3.0")
+      .booleanConf
+      .createWithDefault(false)
+
   val RUNTIME_FILTER_SEMI_JOIN_REDUCTION_ENABLED =
     buildConf("spark.sql.optimizer.runtimeFilter.semiJoinReduction.enabled")
       .doc("When true and if one side of a shuffle join has a selective predicate, we attempt " +
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala
index fd1824055d..e51d0bd874 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala
@@ -69,6 +69,11 @@ case class HadoopFsRelation(
     (location.sizeInBytes * compressionFactor).toLong
   }
 
+  override def estimateSizeInBytes: Long = {
+    val partitionDirectories: Seq[PartitionDirectory] = location.listFiles(Nil, Nil)
+    val files = partitionDirectories.flatMap(_.files)
+    if (files.isEmpty) 0 else files.map(_.getLen).sum
+  }
 
   override def inputFiles: Array[String] = location.inputFiles
 }
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala
index 291b98fb37..cb725b3385 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala
@@ -45,6 +45,11 @@ case class LogicalRelation(
       .getOrElse(Statistics(sizeInBytes = relation.sizeInBytes))
   }
 
+  def estimateStats(): Statistics = {
+    val sizeInBytes = relation.estimateSizeInBytes
+    Statistics(sizeInBytes = sizeInBytes)
+  }
+
   /** Used to lookup original attribute capitalization */
   val attributeMap: AttributeMap[AttributeReference] = AttributeMap(output.map(o => (o, o)))
 
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala
index d70c4b11bc..ca7b8b2c1f 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala
@@ -110,7 +110,7 @@ abstract class PartitioningAwareFileIndex(
         throw new IllegalArgumentException(
           "Datasource with partition do not allow recursive file loading.")
       }
-      prunePartitions(partitionFilters, partitionSpec()).map {
+      prunePartitions(partitionFilters, partitionSpec()).toArray.map {
         case PartitionPath(values, path) =>
           val files: Seq[FileStatus] = leafDirToChildrenFiles.get(path) match {
             case Some(existingDir) =>
@@ -124,7 +124,7 @@ abstract class PartitioningAwareFileIndex(
           }
           PartitionDirectory(values, files)
       }
-    }
+    }.toSeq
     logTrace("Selected files after partition pruning:\n\t" + selectedPartitions.mkString("\n\t"))
     selectedPartitions
   }
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala b/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala
index 63e57c6804..5314669808 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala
@@ -208,6 +208,8 @@ abstract class BaseRelation {
    */
   def sizeInBytes: Long = sqlContext.conf.defaultSizeInBytes
 
+  def estimateSizeInBytes: Long = sizeInBytes
+
   /**
    * Whether does it need to convert the objects in Row to internal representation, for example:
    *  java.lang.String to UTF8String
-- 
2.20.1

