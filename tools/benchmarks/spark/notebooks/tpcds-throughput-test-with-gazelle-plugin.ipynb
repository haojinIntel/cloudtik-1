{
 "cells": [
  {
   "source": [
    "# Run TPC-DS throughput test with gazelle"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Define spark configuration to enable gazelle"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "\n",
    "launcher.conf.set(\"spark.driver.extraClassPath\", \"/home/cloudtik/runtime/benchmark-tools/spark-sql-perf/target/scala-2.12/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar:/home/cloudtik/runtime/oap/oap_jars/gazelle-plugin-1.4.0-spark-3.2.1.jar\") \n",
    "launcher.conf.set(\"spark.executor.extraClassPath\", \"/home/cloudtik/runtime/benchmark-tools/spark-sql-perf/target/scala-2.12/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar:/home/cloudtik/runtime/oap/oap_jars/gazelle-plugin-1.4.0-spark-3.2.1.jar\") \n",
    "launcher.conf.set(\"spark.executor.instances\", \"48\") \n",
    "launcher.conf.set(\"spark.driver.memory\", \"20g\") \n",
    "launcher.conf.set(\"spark.driver.maxResultSize\", \"20g\") \n",
    "launcher.conf.set(\"spark.executor.cores\", \"8\") \n",
    "launcher.conf.set(\"spark.executor.memory\", \"8g\") \n",
    "launcher.conf.set(\"spark.executor.memoryOverhead\", \"384\") \n",
    "launcher.conf.set(\"spark.memory.offHeap.enabled\", \"true\") \n",
    "launcher.conf.set(\"spark.memory.offHeap.size\", \"16g\") \n",
    "launcher.conf.set(\"spark.dynamicAllocation.enabled\", \"false\") \n",
    "launcher.conf.set(\"spark.executorEnv.CC\", \"/home/cloudtik/runtime/oap/bin/x86_64-conda_cos6-linux-gnu-cc\") \n",
    "launcher.conf.set(\"spark.plugins\", \"com.intel.oap.GazellePlugin\") \n",
    "launcher.conf.set(\"spark.executorEnv.LD_LIBRARY_PATH\", \"/home/cloudtik/runtime/oap/lib/\") \n",
    "launcher.conf.set(\"spark.executorEnv.LIBARROW_DIR\", \"/home/cloudtik/runtime/oap/\") \n",
    "launcher.conf.set(\"spark.shuffle.manager\", \"org.apache.spark.shuffle.sort.ColumnarShuffleManager\") \n",
    "launcher.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"false \") \n",
    "launcher.conf.set(\"spark.sql.inMemoryColumnarStorage.batchSize\", \"20480\") \n",
    "launcher.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatc\", \"20480\") \n",
    "launcher.conf.set(\"spark.sql.parquet.columnarReaderBatchSize\", \"20480\") \n",
    "launcher.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10M\") \n",
    "launcher.conf.set(\"spark.sql.broadcastTimeout\", \"3000\") \n",
    "launcher.conf.set(\"spark.sql.crossJoin.enabled\", \"true\") \n",
    "launcher.conf.set(\"spark.sql.columnar.window\", \"true\") \n",
    "launcher.conf.set(\"spark.sql.columnar.sort\", \"true\") \n",
    "launcher.conf.set(\"spark.sql.codegen.wholeStage\", \"true\") \n",
    "launcher.conf.set(\"spark.sql.columnar.codegen.hashAggregate\", \"false\") \n",
    "launcher.conf.set(\"spark.sql.shuffle.partitions\", \"384\") \n",
    "launcher.conf.set(\"spark.kryoserializer.buffer.max\", \"128m\") \n",
    "launcher.conf.set(\"spark.kryoserializer.buffer\", \"32m\") \n",
    "launcher.conf.set(\"spark.oap.sql.columnar.preferColumnar\", \"false\") \n",
    "launcher.conf.set(\"spark.oap.sql.columnar.sortmergejoin.lazyread\", \"true\") \n",
    "launcher.conf.set(\"spark.oap.sql.columnar.sortmergejoin\", \"true\") \n",
    "launcher.conf.set(\"spark.oap.sql.columnar.coreRange\", \"0-31,64-95|32-63,96-127\") \n",
    "launcher.conf.set(\"spark.oap.sql.columnar.joinOptimizationLevel\", \"18\") \n",
    "launcher.conf.set(\"spark.oap.sql.columnar.shuffle.customizedCompression.codec\", \"lz4\") \n",
    "launcher.conf.set(\"spark.executorEnv.ARROW_ENABLE_NULL_CHECK_FOR_GET\", \"false\") \n",
    "launcher.conf.set(\"spark.executorEnv.ARROW_ENABLE_UNSAFE_MEMORY_ACCESS\", \"true\")\n",
    "launcher.conf.set(\"spark.executorEnv.AWS_ACCESS_KEY_ID\", \"xxxxxx\") \n",
    "launcher.conf.set(\"spark.executorEnv.AWS_SECRET_ACCESS_KEY\", \"xxxxxx\")\n"
   ]
  },
  {
   "source": [
    "## Define the benchmark configuration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val streamNumber = 2              // how many streams you want to start \n",
    "val scaleFactor = \"1\"           // data scale 1GB\n",
    "val iterations = 1              // how many times to run the whole set of queries.\n",
    "val format = \"parquet\"          // support parquer or orc\n",
    "// support s3a://s3_bucket, gs://gs_bucket, hdfs://namenode_ip:9000\n",
    "// wasbs://container@storage_account.blob.core.windows.net\n",
    "// abfs://container@storage_account.dfs.core.windows.net\n",
    "val fsdir = \"hdfs://namenode_ip:9000\" \n",
    "val partitionTables = true      // create partition tables\n",
    "val query_filter = Seq()        // Seq() == all queries\n",
    "//val query_filter = Seq(\"q1-v2.4\", \"q2-v2.4\") // run subset of queries\n",
    "val randomizeQueries = true    // run queries in a random order. Recommended for parallel runs.\n",
    "val recreateDatabase = false    // If the previous table creation failed, then this value needs to be set to true\n"
   ]
  },
  {
   "source": [
    "## Create tables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.text.SimpleDateFormat;\n",
    "import java.util.Date\n",
    "import java.util.concurrent.Executors\n",
    "import java.util.concurrent.ExecutorService\n",
    "import java.util.concurrent.TimeUnit\n",
    "import com.databricks.spark.sql.perf.tpcds.TPCDS\n",
    "import com.databricks.spark.sql.perf.Benchmark.ExperimentStatus\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions.{col, lit, substring}\n",
    "\n",
    "val current_time = new SimpleDateFormat(\"yyyy-MM-dd-HH-mm-ss\").format(new Date)\n",
    "// detailed results will be written as JSON to this location.\n",
    "var resultLocation = s\"${fsdir}/shared/data/results/tpcds_${format}/${scaleFactor}/${current_time}\"\n",
    "var databaseName = s\"tpcds_${format}_scale_${scaleFactor}_db\"\n",
    "val use_arrow = true            // when you want to use gazella_plugin to run TPC-DS, you need to set it true.\n",
    "val data_path = s\"${fsdir}/shared/data/tpcds/tpcds_${format}/${scaleFactor}\"\n",
    "\n",
    "\n",
    "if (use_arrow){\n",
    "    resultLocation = s\"${fsdir}/shared/data/results/tpcds_arrow/${scaleFactor}/\"\n",
    "    databaseName = s\"tpcds_arrow_scale_${scaleFactor}_db\"\n",
    "    val tables = Seq(\"call_center\", \"catalog_page\", \"catalog_returns\", \"catalog_sales\", \"customer\", \"customer_address\", \"customer_demographics\", \"date_dim\", \"household_demographics\", \"income_band\", \"inventory\", \"item\", \"promotion\", \"reason\", \"ship_mode\", \"store\", \"store_returns\", \"store_sales\", \"time_dim\", \"warehouse\", \"web_page\", \"web_returns\", \"web_sales\", \"web_site\")\n",
    "    if (spark.catalog.databaseExists(s\"$databaseName\")) {\n",
    "        if (!recreateDatabase) {\n",
    "            println(s\"Using existing $databaseName\")\n",
    "        } else {\n",
    "            println(s\"$databaseName exists, now drop and recreate it...\")\n",
    "            sql(s\"drop database if exists $databaseName cascade\")\n",
    "            sql(s\"create database if not exists $databaseName\").show\n",
    "        }\n",
    "    } else {\n",
    "        println(s\"$databaseName doesn't exist. Creating...\")\n",
    "        sql(s\"create database if not exists $databaseName\").show\n",
    "    }\n",
    "    sql(s\"use $databaseName\").show\n",
    "    for (table <- tables) {\n",
    "        if (spark.catalog.tableExists(s\"$table\")){\n",
    "            println(s\"$table exists.\")\n",
    "        }else{\n",
    "            spark.catalog.createTable(s\"$table\", s\"$data_path/$table\", \"arrow\")\n",
    "        }\n",
    "    }\n",
    "    if (partitionTables) {\n",
    "        for (table <- tables) {\n",
    "            try{\n",
    "                sql(s\"ALTER TABLE $table RECOVER PARTITIONS\").show\n",
    "            }catch{\n",
    "                case e: Exception => println(e)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "} else {\n",
    "    // Check whether the database is created, we create external tables if not\n",
    "    val databaseExists = spark.catalog.databaseExists(s\"$databaseName\")\n",
    "    if (databaseExists && !recreateDatabase) {\n",
    "        println(s\"Using existing $databaseName\")\n",
    "    } else {\n",
    "        if (databaseExists) {\n",
    "            println(s\"$databaseName exists, now drop and recreate it...\")\n",
    "            sql(s\"drop database if exists $databaseName cascade\")\n",
    "        } else {\n",
    "            println(s\"$databaseName doesn't exist. Creating...\")\n",
    "        }\n",
    "\n",
    "        import com.databricks.spark.sql.perf.tpcds.TPCDSTables\n",
    "\n",
    "        val tables = new TPCDSTables(spark.sqlContext, \"\", s\"${scaleFactor}\", false)\n",
    "        tables.createExternalTables(data_path, format, databaseName, overwrite = true, discoverPartitions = partitionTables)\n",
    "    }\n",
    "}\n",
    "\n",
    "val timeout = 60 // timeout in hours\n"
   ]
  },
  {
   "source": [
    "## Run queries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql(s\"use $databaseName\")\n",
    "import com.databricks.spark.sql.perf.tpcds.TPCDS\n",
    "val tpcds = new TPCDS (sqlContext = spark.sqlContext)\n",
    "def queries(stream: Int) = {\n",
    "  val filtered_queries = query_filter match {\n",
    "    case Seq() => tpcds.tpcds2_4Queries\n",
    "    case _ => tpcds.tpcds2_4Queries.filter(q => query_filter.contains(q.name))\n",
    "  }\n",
    "  if (randomizeQueries && stream != 0) scala.util.Random.shuffle(filtered_queries) else filtered_queries\n",
    "}\n",
    "\n",
    "class ThreadStream(experiment:ExperimentStatus, i:Int) extends Thread{\n",
    "    override def run(){\n",
    "        println(\"stream_\" + i + \" has started...\")\n",
    "        println(experiment.toString)\n",
    "        experiment.waitForFinish(timeout*60*60)\n",
    "        println(\"stream_\" + i + \" has finished.\")\n",
    "    }\n",
    "}\n",
    "\n",
    "val threadPool:ExecutorService = Executors.newFixedThreadPool(streamNumber)\n",
    "val experiments:Array[ExperimentStatus] = new Array[ExperimentStatus](streamNumber)\n",
    "\n",
    "try {\n",
    "    for(i <- 0 to (streamNumber - 1)){\n",
    "        experiments(i) = tpcds.runExperiment(\n",
    "            queries(i), \n",
    "            iterations = iterations,\n",
    "            resultLocation = resultLocation,\n",
    "            tags = Map(\"runtype\" -> \"benchmark\", \"database\" -> databaseName, \"scale_factor\" -> scaleFactor)\n",
    "        )\n",
    "        threadPool.execute(new ThreadStream(experiments(i), i))\n",
    "    }\n",
    "}finally{\n",
    "    threadPool.shutdown()\n",
    "    threadPool.awaitTermination(Long.MaxValue, TimeUnit.NANOSECONDS)\n",
    "}\n",
    "\n",
    "val summary_dfs = new Array[DataFrame](streamNumber)\n",
    "for(i <- 0 to (streamNumber - 1)){\n",
    "   summary_dfs(i) = experiments(i).getCurrentResults.withColumn(\"Name\", substring(col(\"name\"), 2, 100)).withColumn(\"Runtime\", (col(\"parsingTime\") + col(\"analysisTime\") + col(\"optimizationTime\") + col(\"planningTime\") + col(\"executionTime\")) / 1000.0).select('Name, 'Runtime).agg(sum(\"Runtime\")).withColumn(\"stream\", lit(\"stream_\" + i)).select(\"stream\", \"sum(Runtime)\")\n",
    "}\n",
    "var summary_df = summary_dfs(0)\n",
    "for (i <- 0 to (streamNumber - 1)){\n",
    "    if (i != 0) {\n",
    "        summary_df = summary_df.union(summary_dfs(i))\n",
    "    }   \n",
    "}\n",
    "summary_df = summary_df.union(summary_df.agg(max(\"sum(Runtime)\")).withColumnRenamed(\"max(sum(Runtime))\",\"sum(Runtime)\").withColumn(\"stream\", lit(\"max_stream\")).select(\"stream\", \"sum(Runtime)\"))\n",
    "summary_df.show()\n",
    "\n",
    "// Save the performance summary dataframe to a CSV file with a specified file name\n",
    "val finalResultPath = s\"${resultLocation}/summary\"\n",
    "summary_df.repartition(1).write.option(\"header\", \"true\").mode(\"overwrite\").csv(finalResultPath)\n",
    "\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "import java.net.URI\n",
    "\n",
    "val fs = FileSystem.get(URI.create(finalResultPath), sc.hadoopConfiguration)\n",
    "val file = fs.globStatus(new Path(s\"$finalResultPath/*.csv\"))(0).getPath().getName()\n",
    "val srcPath=new Path(s\"$finalResultPath/$file\")\n",
    "val destPath= new Path(s\"$finalResultPath/summary.csv\")\n",
    "fs.rename(srcPath, destPath)\n",
    "\n",
    "for(i <- 0 to (streamNumber - 1)){\n",
    "   println(s\"stream_${i} result is saved to ${experiments(i).resultPath}\")\n",
    "}\n",
    "\n",
    "println(s\"Performance summary is saved to ${destPath}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}